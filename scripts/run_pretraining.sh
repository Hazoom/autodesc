#!/usr/bin/env bash
python src/bertcode/bert/run_pretraining.py --input_file=data/cleaned_titles_tf_examples.tfrecord --output_dir=data/cleaned_titles_pretraining_output --do_train=True --do_eval=True --bert_config_file=bert/models/uncased_L-12_H-768_A-12/bert_config.json --init_checkpoint=bert/models/uncased_L-12_H-768_A-12/bert_model.ckpt --train_batch_size=32 --max_seq_length=64 --max_predictions_per_seq=10 --num_train_steps=1000 --num_warmup_steps=30 --learning_rate=2e-5